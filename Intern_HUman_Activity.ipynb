{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0352db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading dataset\"\"\"\n",
    "\n",
    "test = pd.read_csv('C:\\\\Users\\\\CYBER ACE\\\\Desktop\\\\project\\\\internship_Project\\\\Program 3 (Machine learning) activity data\\\\test.csv')\n",
    "train = pd.read_csv('C:\\\\Users\\\\CYBER ACE\\\\Desktop\\\\project\\\\internship_Project\\\\Program 3 (Machine learning) activity data\\\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_train=train.isna()\n",
    "missing_value_test=test.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c585d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in training set:(%)\")\n",
    "print((missing_value_train/len(train))*100)\n",
    "print(\"Missing values in test set:(%)\")\n",
    "print((missing_value_test/len(test))*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ad066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cor=test.corr()\n",
    "#plt.figure(figsize=(10,6))\n",
    "#sns.heatmap(cor,annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed0d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Obtain the train and test data\"\"\"\n",
    "\n",
    "y_train = train.Activity\n",
    "X_train = train.drop(['subject', 'Activity'], axis=1)\n",
    "y_test = test.Activity\n",
    "X_test = test.drop(['subject', 'Activity'], axis=1)\n",
    "print('Training data size : ', X_train.shape)\n",
    "print('Test data size : ', X_test.shape)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838fdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)\n",
    "\n",
    "labels=['LAYING', 'SITTING','STANDING','WALKING','WALKING_DOWNSTAIRS','WALKING_UPSTAIRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c88b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to plot the confusion matrix\"\"\"\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "plt.rcParams[\"font.family\"] = 'DejaVu Sans'\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ca307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"function to run any model specified\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "def perform_model(model, X_train, y_train, X_test, y_test, class_labels, cm_normalize=True, \\\n",
    "                 print_cm=True, cm_cmap=plt.cm.Greens):\n",
    "    \n",
    "    \n",
    "    # to store results at various phases\n",
    "    results = dict()\n",
    "    \n",
    "    # time at which model starts training \n",
    "    train_start_time = datetime.now()\n",
    "    print('training the model..')\n",
    "    model.fit(X_train, y_train)\n",
    "    print('Done \\n \\n')\n",
    "    train_end_time = datetime.now()\n",
    "    results['training_time'] =  train_end_time - train_start_time\n",
    "    print('training_time(HH:MM:SS.ms) - {}\\n\\n'.format(results['training_time']))\n",
    "    \n",
    "    \n",
    "    # predict test data\n",
    "    print('Predicting test data')\n",
    "    test_start_time = datetime.now()\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_end_time = datetime.now()\n",
    "    print('Done \\n \\n')\n",
    "    results['testing_time'] = test_end_time - test_start_time\n",
    "    print('testing time(HH:MM:SS:ms) - {}\\n\\n'.format(results['testing_time']))\n",
    "    results['predicted'] = y_pred\n",
    "   \n",
    "\n",
    "    # calculate overall accuracty of the model\n",
    "    accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    # store accuracy in results\n",
    "    results['accuracy'] = accuracy\n",
    "    print('---------------------')\n",
    "    print('|      Accuracy      |')\n",
    "    print('---------------------')\n",
    "    print('\\n    {}\\n\\n'.format(accuracy))\n",
    "    \n",
    "    \n",
    "    # confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    results['confusion_matrix'] = cm\n",
    "    if print_cm: \n",
    "        print('--------------------')\n",
    "        print('| Confusion Matrix |')\n",
    "        print('--------------------')\n",
    "        print('\\n {}'.format(cm))\n",
    "        \n",
    "    # plot confusin matrix\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.grid(b=False)\n",
    "    plot_confusion_matrix(cm, classes=class_labels, normalize=True, title='Normalized confusion matrix', cmap = cm_cmap)\n",
    "    plt.show()\n",
    "    \n",
    "    # get classification report\n",
    "    print('-------------------------')\n",
    "    print('| Classifiction Report |')\n",
    "    print('-------------------------')\n",
    "    classification_report = metrics.classification_report(y_test, y_pred)\n",
    "    # store report in results\n",
    "    results['classification_report'] = classification_report\n",
    "    print(classification_report)\n",
    "    \n",
    "    # add the trained  model to the results\n",
    "    results['model'] = model\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76176c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Method to print the gridsearch Attributes\"\"\"\n",
    "\n",
    "def print_grid_search_attributes(model):\n",
    "    # Estimator that gave highest score among all the estimators formed in GridSearch\n",
    "    print('--------------------------')\n",
    "    print('|      Best Estimator     |')\n",
    "    print('--------------------------')\n",
    "    print('\\n\\t{}\\n'.format(model.best_estimator_))\n",
    "\n",
    "\n",
    "    # parameters that gave best results while performing grid search\n",
    "    print('--------------------------')\n",
    "    print('|     Best parameters     |')\n",
    "    print('--------------------------')\n",
    "    print('\\tParameters of best estimator : \\n\\n\\t{}\\n'.format(model.best_params_))\n",
    "\n",
    "\n",
    "    #  number of cross validation splits\n",
    "    print('---------------------------------')\n",
    "    print('|   No of CrossValidation sets   |')\n",
    "    print('--------------------------------')\n",
    "    print('\\n\\tTotal numbre of cross validation sets: {}\\n'.format(model.n_splits_))\n",
    "\n",
    "\n",
    "    # Average cross validated score of the best estimator, from the Grid Search \n",
    "    print('--------------------------')\n",
    "    print('|        Best Score       |')\n",
    "    print('--------------------------')\n",
    "    print('\\n\\tAverage Cross Validate scores of best estimator : \\n\\n\\t{}\\n'.format(model.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8034769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1. Logistic Regression\"\"\"\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# start Grid search\n",
    "parameters = {'C':[0.01, 0.1, 1, 10, 20, 30], 'penalty':['l2','l1']}\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "log_reg_grid = GridSearchCV(log_reg, param_grid=parameters, cv=3, verbose=1, n_jobs=-1)\n",
    "log_reg_grid_results =  perform_model(log_reg_grid, X_train, y_train, X_test, y_test, class_labels=labels)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.grid(b=False)\n",
    "plot_confusion_matrix(log_reg_grid_results['confusion_matrix'], classes=labels, cmap=plt.cm.Greens, )\n",
    "plt.show()\n",
    "\n",
    "# observe the attributes of the model \n",
    "print_grid_search_attributes(log_reg_grid_results['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2. Linear SVC\"\"\"\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "parameters = {'C':[0.125, 0.5, 1, 2, 8, 16]}\n",
    "lr_svc = LinearSVC(tol=0.00005)\n",
    "lr_svc_grid = GridSearchCV(lr_svc, param_grid=parameters, n_jobs=-1, verbose=1)\n",
    "lr_svc_grid_results = perform_model(lr_svc_grid, X_train, y_train, X_test, y_test, class_labels=labels)\n",
    "\n",
    "print_grid_search_attributes(lr_svc_grid_results['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b699250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3. Decision Tree Classifier\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'max_depth':np.arange(3,10,2)}\n",
    "dt = DecisionTreeClassifier()\n",
    "dt_grid = GridSearchCV(dt,param_grid=parameters, n_jobs=-1)\n",
    "dt_grid_results = perform_model(dt_grid, X_train, y_train, X_test, y_test, class_labels=labels)\n",
    "print_grid_search_attributes(dt_grid_results['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f137d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"4. Comparing all models\"\"\"\n",
    "\n",
    "print('\\n                     Accuracy     Error')\n",
    "print('                     ----------   --------')\n",
    "print('Logistic Regression : {:.04}%       {:.04}%'.format(log_reg_grid_results['accuracy'] * 100,\\\n",
    "                                                  100-(log_reg_grid_results['accuracy'] * 100)))\n",
    "\n",
    "print('Linear SVC          : {:.04}%       {:.04}% '.format(lr_svc_grid_results['accuracy'] * 100,\\\n",
    "                                                        100-(lr_svc_grid_results['accuracy'] * 100)))\n",
    "\n",
    "print('DecisionTree        : {:.04}%        {:.04}% '.format(dt_grid_results['accuracy'] * 100,\\\n",
    "                                                        100-(dt_grid_results['accuracy'] * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252286d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
